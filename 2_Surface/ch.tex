\chapter{A Dependent Type System}
\label{chapter:Surface}
\thispagestyle{myheadings}

% \section{Introduction}

Despite the usability issues this thesis hopes to correct, dependent type systems are still one of the most promising technologies for correct programming.
Since proofs are programs, there is no additional syntax for programmers to learn.
The proof system is predictable from the perspective of a functional programmer. 
\todo{awk sentence}

The \textbf{surface type system}\todo{is there a better name then surface lang?} presented in this chapter provides a minimal dependent type system.
The rules of the type system are intended to be as simple as possible and compatible with other well studied intentional dependent type theories.
It has several (but not all) of the standard properties of dependent type theory.
As much as possible, the syntax uses standard modern notation\footnote{
  Several alternative syntaxes exist in the literature.
  In this document the typed polymorphic identity function is written, $\lambda-\,x\Rightarrow x\ :\,\left(X:\star\right)\rightarrow X\rightarrow X$.
  In \cite{10.1016/0890-5401(88)90005-3} it might be written $\left(\lambda X:\star\right)\left(\lambda x:X\right)x\ :\,\left[X:\star\right]\left[x:X\right]X$.
  In \cite{HoTTbook} it might be written $\lambda X.\lambda x.x\ :\,\underset{\left(X:\mathcal{U}\right)}{\prod}X\rightarrow X$.}\todo{Martin Loff/Hoff (TODO pr notation)}.

The surface type system will serve both as foundation for later chapters and a self contained technical introduction to dependent types.
Even when useing the full system described in later chapters, programmers will only need to think about the surface system.
By design, the machinery that deals with equality addressed in later chapters will be invisible to programers.
Everything presented in later chapters is designed to reinforce an understanding of the surface type system, and make it easier to use.

% overview, What
The surface language deviates from a standard dependent type theory to include features for programming at the expense of logical correctness.
Specifically the language allows general recursion, since general recursion is useful for programmers.
\Tit{} is also supported since it simplifies the system, and makes the meta-theory slightly easier.
Despite this, type soundness is achievable, and a practical type checking system is given.

Though similar systems have been studied over the last few decades this chapter aims to give a self contained presentation, along with examples.
The surface language has been a good platform to conduct research into full spectrum dependent type theory, and hopefully this exposition will be a helpful introduction for other researchers.

\section{Surface Language Syntax}

% What syntax
The syntax for the surface language is in \Fref{surface-pre-syntax}.
The syntax supports: variables, type annotations, a single type universe, dependent function types, recursive dependent functions, and function applications.
Type annotations are written with two colons to differentiate it from the formal typing judgments that will appear more frequently in this text.
In the implemented language a user of the programming language would use a single colon.

There is no distinction between types and terms in the syntax\footnote{
  terms and types are usually separated, except in the syntax of full-spectrum dependent type systems where separating them would require many redundant rules.
  }, both are referred to as expressions.
However, capital metavariables are used in positions that are intended as types, and lowercase metavariables are used when an expression is intended to be a term.
For instance, in annotation syntax where $m::M$ means $m$ can be a term and $M$ should be a type.
% Metavariables that intend to quantify equivalent terms will be noted with primes or subscripts, 
% Metavariables that type other metavairbles will often use the same letter

\todo{more about f being recursive?}
\todo{extrinsic}

\begin{figure}
% \begin{grammar}{x,y,z,f}{variable identifier}
% \end{grammar}
% \begin{grammar}{\Gamma\Coloneqq}{type context}
%   \mathbf{\lozenge}                        & empty context \\
%   \mathbf{Gamma,x:M}                       & extend context with x of type M \\
% \end{grammar}

\begin{tabular}{lcll}
\multicolumn{4}{l}{variable identifiers,}\tabularnewline
\multicolumn{4}{l}{$x,y,z,f$}\tabularnewline
\multicolumn{4}{l}{expressions,}\tabularnewline
$m,n,M,N$ & $\Coloneqq$ & $x$ & variable\tabularnewline
  & $|$ & $m::M$ & annotation\tabularnewline
  & $|$ & $\star$ & type universe\tabularnewline
  & $|$ & $\left(x:M\right)\rightarrow N$ & function type\tabularnewline
  & $|$ & $\mathsf{fun}\,f\,x\Rightarrow m$ & function\tabularnewline
  & $|$ & $m\,n$ & application\tabularnewline
\multicolumn{4}{l}{type contexts,}\tabularnewline
$\Gamma$ & $\Coloneqq$ & $\lozenge$ $|$ $\Gamma,x:M$ & \tabularnewline
\end{tabular}\caption{Surface Language Syntax}
\label{fig:surface-pre-syntax}
\end{figure}
  
Several standard abbreviations are listed in \Fref{surface-pre-syntax-abrev}.
\begin{figure}
\begin{tabular}{lclll}
$\left(x:M\right)\rightarrow N$ & written & $M\rightarrow N$ & when  & $x\notin fv\left(N\right)$\tabularnewline
$\mathsf{fun}\,f\,x\Rightarrow m$ & written & $\lambda x\Rightarrow m$ & when  & $f\notin fv\left(m\right)$\tabularnewline
$...\,x\Rightarrow\lambda y\Rightarrow m$ & written & $...\,x\,y\Rightarrow m$ &  & \tabularnewline
$x$ & written & $-$ & when  & $x\notin fv\left(m\right)$ when $x$ binds $m$\tabularnewline
\end{tabular}
  
where $fv$ is a function that returns the set of free variables in an expression
\caption{Surface Language Abbreviations}
\label{fig:surface-pre-syntax-abrev}
\end{figure}

\input{2_Surface/2_Examples}
\input{2_Surface/3_TAS}

\subsection{\Bidir{} Type Checking}

\textbf{\Bidir{} type checking} is a popular form of lightweight type inference, which strikes a good compromise between the required type annotations and the simplicity of the procedure, allowing for localized errors\footnote{\cite{christiansen2013\bidir{}} is a good tutorial, \cite{10.1145/3450952} is a survey of the technique}.
In the usual \bidir{} typing schemes, annotations are only required at the top-level, or around a function that is directly applied to an argument\footnote{more generally when an elimination reduction is possible.}.
For example $(\lambda x\Rightarrow x+x)7$ would need to be written $\left((\lambda x\Rightarrow x+x)::\mathbb{N}\rightarrow\mathbb{N}\right)7$.
Since programers rarely write functions that are immediately evaluated, this style of type checking usually only needs top level functions to be annotated\footnote{Even in Haskell, with full Hindley-Milner type inference, top level type annotations are encouraged.}\todo{ref style guide.
the point was similarly made in Agda thesis.}.
In fact, almost every example in \Fref{surface-examples} has enough annotations to type check \bidir{}{}ly without further information.

\todo{note popularity}

This is accomplished by breaking the typing judgments into two mutual judgments:
\begin{itemize}
\item \textbf{Type Inference} where type information propagates out of a term, $\overrightarrow{\,:\,}$ in our notation. 
\item \textbf{Type Checking} judgments where a term is checked against a type, $\overleftarrow{\,:\,}$ in our notation. 
\end{itemize}
This allows typing information to flow from the "outside in" for type checking judgments and "inside out" for the type inference judgments.
A check can be induced manually with a type annotation.
When an inference meets a check, a conversion verifies that the types are definitionally equal.
This has the advantage of precisely limiting where the $\operatorname{ty-conv}$ rule is needed, since conversion checking is usually an inefficient part of dependent type checking.

This enforced flow of information results in a system that localizes type errors.
If a type was inferred, it was unique from the term, so it can be used freely.
Checking judgments force terms that could have multiple typings in the \ac{TAS} to have at most one type.

\begin{figure}
\[
\frac{x:M\in\Gamma}{\Gamma\vdash x\overrightarrow{\,:\,}M}\operatorname{\overrightarrow{ty}-var}
\]
\[
\frac{\,}{\Gamma\vdash\star\overrightarrow{\,:\,}\star}\operatorname{\overrightarrow{ty}-\star}
\]
\[
\frac{\Gamma\vdash m\overleftarrow{\,:\,}M}{\Gamma\vdash m::M\overrightarrow{\,:\,}M}\operatorname{\overrightarrow{ty}-::}
\]
\[
\frac{\Gamma\vdash M\overleftarrow{\,:\,}\star\quad\Gamma,x:M\vdash N\overleftarrow{\,:\,}\star}{\Gamma\vdash\left(x:M\right)\rightarrow N\overrightarrow{\,:\,}\star}\operatorname{\overrightarrow{ty}-\mathsf{fun}-ty}
\]
\[
\frac{\Gamma\vdash m\overrightarrow{\,:\,}\left(x:N\right)\rightarrow M\quad\Gamma\vdash n\overleftarrow{\,:\,}N}{\Gamma\vdash m\,n\overrightarrow{\,:\,}M\left[x\coloneqq n\right]}\operatorname{\overrightarrow{ty}-\mathsf{fun}-app}
\]
\[
\frac{\Gamma,f:\left(x:N\right)\rightarrow M,x:N\vdash m\overleftarrow{\,:\,}M}{\Gamma\vdash\mathsf{fun}\,f\,x\Rightarrow m\overleftarrow{\,:\,}\left(x:N\right)\rightarrow M}\operatorname{\overleftarrow{ty}-\mathsf{fun}}
\]
\[
\frac{\Gamma\vdash m\overrightarrow{\,:\,}M\quad M\equiv M'}{\Gamma\vdash m\overleftarrow{\,:\,}M'}\operatorname{\overleftarrow{ty}-conv}
\]

\caption{Surface Language \Bidir{} Typing Rules}
\label{fig:surface-bityping-rules}
\end{figure}

% review the typing rule 
The surface language supports \bidir{} type-checking over the pre-syntax with the rules in \Fref{surface-bityping-rules}.
The rules are almost the same as before except that typing direction is now explicit in the judgment.

As mentioned, \bidir{} type checking handles higher order functions very well.
For instance, the expression $\vdash(\lambda x\Rightarrow x\,(\lambda y\Rightarrow y)\,2)\overleftarrow{\,:\,}\left(\left(\mathbb{N}\rightarrow\mathbb{N}\right)\rightarrow\mathbb{N}\rightarrow\mathbb{N}\right)\rightarrow\mathbb{N}$ checks because $\vdash(\lambda y\Rightarrow y)\overleftarrow{\,:\,}\left(\mathbb{N}\rightarrow\mathbb{N}\right)$ and $\vdash2\overleftarrow{\,:\,}\mathbb{N}$.

Unlike the undirected judgments of the Type Assignment System, the inference rule of the \bidir{} system does not convert
The inference, it is unique up to syntax!
For example $x:Vec\,3\vdash x\overrightarrow{\,:\,}Vec\,3$, but $x:Vec\,3\cancel{\vdash}x\overrightarrow{\,:\,}Vec\,\left(1+2\right)$.
This could cause unexpected behavior around function applications.
For instance, if $\Gamma\vdash m\overrightarrow{\,:\,}\mathbb{N}\rightarrow\mathbb{N}$ then $\Gamma\vdash m\:7\overrightarrow{\,:\,}\mathbb{N}$ will infer, but only because the $\rightarrow$ is in the head position of the type $\mathbb{N}\underline{\rightarrow}\mathbb{N}$.
If $\Gamma\vdash m\overrightarrow{\,:\,}\left(\mathbb{N}\rightarrow\mathbb{N}::\star\right)$ then $::$ is in the head position of $\mathbb{N}\rightarrow\mathbb{N}\underline{::}\star$ and $\Gamma\cancel{\vdash}m\ 7\overrightarrow{\,:\,}\mathbb{N}$ will will not infer.

The similar issue is possible around check rules around function definitions.
For instance, $\vdash\left((\lambda x\Rightarrow x)::\mathbb{N}\rightarrow\mathbb{N}\right)\ \overrightarrow{\,:\,}\mathbb{N}\rightarrow\mathbb{N}$ will infer, but if computation blocks the $\rightarrow$ from being in the head position, inference will be impossible.
As in the expression, $\left((\lambda x\Rightarrow x)::\left(\mathbb{N}\rightarrow\mathbb{N}\underline{::}\star\right)\right)$ which will not infer.

For these reasons, realistic implementations will often evaluate the types needed for $\overleftarrow{ty}-\mathsf{fun}$, and $\operatorname{\overrightarrow{ty}-\mathsf{fun}-app}$ into weak head normal form\footnote{as in \cite{COQUAND1996167}}.
More advanced \bidir{} implementations such as Agda\cite{norell2007towards} even perform unification as part of their \bidir{} type checking.

\todo[inline]{alternative listed in appendix}
\todo[inline]{what else does the algorithm infer not listed here}
\todo[inline]{More about extending the system so constraint solving can happen under a check judgment }
\todo[inline]{Clearly explain why this is needed for the cast system, annotating every var is cumbersome, constraint solving is iffy when things may be undecidable}

This document opts for the simplest possible presentation of \bidir{} type checking.
There will always be ways to make type inference more powerful, at the cost of complexity.

\subsection{The \Bidir{} System is Type Sound}

It is possible to prove \bidir{} type systems are type sound directly\cite{nanevski2005dependent}.
But it would be difficult for the system described here since type annotations evaluate away, complicating preservation.
Alternatively we can show that a \bidir{} typing judgment implies a type assignment system typing judgment.

\begin{thm}
\Bidir{} implies \ac{TAS}

if $\Gamma\vdash m\overrightarrow{\,:\,}M$ then $\Gamma\vdash m\,:\,M$

if $\Gamma\vdash m\overleftarrow{\,:\,}M$ then $\Gamma\vdash m\,:\,M$
\end{thm}

\begin{proof}
by mutual induction on the \bidir{} typing derivations.
\end{proof}
Therefore the \bidir{} system is also type sound.

\subsection{The \ac{TAS} System is weakly annotatable by the \Bidir{} System}

In \Bidir{} systems, \textbf{annotatability}\footnote{also called \textbf{completeness}} is the property that any expression that types in a \ac{TAS} will type in the \bidir{} system with only additional annotations.
This property doesn't exactly hold for the \bidir{} system presented here.
For instance, $\vdash\left((\lambda x\Rightarrow x)::\left(\mathbb{N}\rightarrow\mathbb{N}::\star\right)\right)$ type checks in the \ac{TAS} system, but no amount of annotations will make it check in the \bidir{} system.
Instead we can show that the \bidir{} system does not preclude any computation available in the \ac{TAS}, though annotations may need to be added (or removed).% no need to remove if properly \bidir{}).
We will call this property \textbf{weak annotatability}.
\begin{thm}
weak annotatability.

if $\Gamma\vdash m\,:\,M$ then $\Gamma\vdash m'\overleftarrow{\,:\,}M'$, $m\equiv m'$ and $M\equiv M'$ 

if $\Gamma\vdash m\,:\,M$ then $\Gamma\vdash m'\overrightarrow{\,:\,}M'$ , $m\equiv m'$ and $M\equiv M'$
\end{thm}

\begin{proof}
by induction on the typing derivation, adding and removing annotations at each step that are convertible with the original $m$
\end{proof}
\todo[inline]{slight changes have been made, double check this}

\subsection{Absent Logical Properties}

When type systems are used as logics, it is desirable that
\begin{itemize}
\item There exists a type that is uninhabited in the empty context, so the system is \textbf{logically consistent}\footnote{also called \textbf{logically sound}}.
\item Type checking is decidable.
\end{itemize}
Neither the \ac{TAS} system or the \Bidir{} systems has these properties\footnote{These properties are usually shown by showing that the computation that generates definitional equality is normalizing.
A proof for a more logical system can be found in Chapter 4\cite{luo1994computation}.
Another excellent tutorial can be found in Chapter 2 in \cite{casinghino2014combiningthesis}}.
% aparently that note from Chris Casinghino is dead http://prosecco.gforge.inria.fr/personal/hritcu/temp/snforcc.pdf

\subsubsection{Logical Inconsistency}

The surface language is logically inconsistent, since every type is inhabited.

\begin{example}
Every Type is Inhabited (by recursion)

$\mathsf{fun}\,f\,x\Rightarrow f\,x\qquad:\perp_{c}$
\end{example}

It is possible to encode Girard's paradox, producing another source of logical unsoundness.
\begin{example}
Every Type is Inhabited (by \tit{})

\todo{full example}

\todo{cite stuff (see https://stackoverflow.com/questions/18178999/do-agda-programs-necessarily-terminate), of course this hapens in Girard's french thesis}
\end{example}

A subtle form of recursive behavior can be built out of Gerard's paradox\cite{Reinhold89typecheckingis}, but this behavior is no worse than the unrestricted recursion already allowed.

% I am unaware of anyone accidentally deriving a falsehood from \tit{}.

Operationally, logical inconsistency will be recognized by programmers as non-termination.
Non-termination seems not to matter for programming languages in practice.
For instance, in ML the type $\mathtt{f:Int->Int}$ does not imply the termination of $\mathtt{f\,2}$.
While unproductive non-termination is always a bug, it seems an easy bug to detect and fix when it occurs.
In mainstream languages, types help to communicate the intent of termination, even though termination is not guaranteed by the type system.
Importantly, no desirable computation is prevented in order to preserve logical consistency.
There will never be a way to allow all the terminating computations and exclude all the nonterminating computations.
A tradeoff must be made, and programmers likely care more about having all possible computations than preventing non-termination.
Therefore, logical unsoundness seems suitable for a dependently typed programming language.

\todo{argue from the Blum proof?  Allowing non-termination makes writing termination programs easier.}

\todo{add ref to inequalities}

While the surface language supports proofs, not every term typed in the surface language is a proof.
Terms can still be called proofs as long as the safety of recursion and \tit{} are checked externally.
In this sense, the listed example inequalities are proofs, as they make no use of general recursion (so all recursions are well founded) and universes are used in a safe way (universe hierarchies could be assigned).
In an advanced implementation, an automated process could supply warnings when constructs are used in potentially unsafe ways.
Traditional software testing can be used to discover if there are actual proof bugs.
Even though the type system is not logically consistent, type checking still eliminates a large class of possible mistakes.
While it is possible to make a subtle error, it is easier to make an error in a paper and pencil proofs, or in typeset \LaTeX.

Finally by separating non-termination concerns from the core of the theory, this architecture is resilient to change.
If the termination checker is updated in Coq, there is some chance older proof scripts will no longer type check.
With the architecture proposed here, code will always have the same static and dynamic behavior, though some warnings might appear or disappear.

\subsubsection{Type Checking is Undecidable}
\begin{thm}
Type Checking is Undecidable
\end{thm}

\begin{proof}
Given a thunk $f:Unit$ defined in PCF, it can be encoded into the surface system as a thunk $f':Unit_{c}$, such that if $f$ reduces to the canonical $Unit$ then $f'\Rrightarrow_{\ast}\lambda A.\lambda a.a$ 

$\vdash\star:f'\,\star\,\star$ type-checks by conversion exactly when $f$ halts.

If there is a procedure to decide type checking we can decide exactly when any PCF function halts.
Since checking if a PCF function halts is undecidable, type checking here is undecidable.

\end{proof}
Decidability of type checking is often used as a proxy for efficient typechecking.

Again this the root of the problem is the non-termination that results by allowing as many computations as possible, which seem necessary in a realistic programming language.

Luckily undecidability of type checking is not as bad as it sounds for several reasons.
First, the pathological terms that cause non-terminating conversion are rarely created on purpose.
In the \bidir{} system, conversion checks will only happen at limited positions, and it is possible to use a counter to warn or give errors at code positions that do not convert because normalization takes too long.
Heuristic methods of conversion checking seem to work well enough in practice even without a counter.
It is also possible to embed proofs of conversion directly into the syntax\cite{sjoberg2012irrelevance}.

Many dependent type systems, such as Agda, Coq, and Lean, aspire to decidable type checking.
However these systems allow extremely fast growing functions to be encoded (such as Ackerman's function).
A fast growing function can generate a very large index that can be used to check some concrete but unpredictable property, (how many Turing machines whose code is smaller then $n$ halt in $n$ steps?).
When this kind of computation is lifted to the type level, type checking is computationally infeasible, to say the least.

\todo{make sure I'm not missing any langs, C\#...?}

Many mainstream programming languages have undecidable type checking.
If a language admits a sufficiently powerful macro or preprocessor system that can modify typing, this would make type checking undecidable (this makes the type system of C, C++\footnote{apparently even the grammar of C++ is undecidable}, Scala, and Rust undecidable).
Unless type features are considered very carefully, they can often create undecidable type checking (Java generics, C++ templates, Scala implicit parameters\footnote{without a maximum search depth} and OCaml modules, make type checking undecidable in those languages).
Haskell may be the most popular statically typed language with decidable type checking (and even then popular GHC compiler flags make type checking undecidable).
Even the Hindley-Milner type checking algorithm that underlies Haskell and ML, has a worst case complexity that is double exponential, which under normal circumstances would be considered intractable.

In practice these theoretical concerns are irrelevant since programmers are not giving the compiler ``worst case'' code.
Even if they did, the worst that can happen is the type checking will hang in the compilation process.
When this happens in a mainstream language, programmers can fix their code, modify or remove macros, or add typing annotations.
Programmers in conventional languages are already entrusted with almost unlimited power over their programming environments.
Programs regularly delete files, read and modify sensitive information, and send emails (some of these are even possible from within the language's macro systems).
Relatively speaking, undecidable type checking is not a programmer's biggest concern.
\todo{a little awk}

Most importantly for the system described in this thesis, users are expected to use the elaboration procedure defined in the next chapter that will bypass the type checking described here.
\todo{than why review?}
% That elaboration procedure is also undecidable, but only for extremely pathological terms.

\section{Related work}

\subsection{Bad logics, ok programming languages?}

Unsound logical systems that work as programming languages go back to at least Church's lambda calculus which was originally intended to be part of a foundation for mathematics\footnote{``There may, indeed, be other applications of the system than its use as a logic.''{[}Church, 1932, p.349, A Set of Postulates for the Foundation of Logic{]}}.\todo{proper citation}
In the 1970s, Per Martin-L{\"o}f proposed a system with \tit{}\todo{cite it?} that was shown logically unsound by Girard (as described in the introduction in \cite{Martin-Lof-1972}).
In the 1980s, Cardelli explored the domain semantics of a system with general recursive dependent functions and \tit{}\cite{cardelli1986polymorphic}.
Independently, Viggo and Stoltenberg-Hansen\cite{PALMGREN1990135} explored the domain semantics of Martin-L{\"o}f's type theory with a fixed point operator.
\todo{Per Martin-L{\"o}f anticipated this work in unpublished lectures and an Abstract.}

The first progress and preservation style proof of type soundness for a language with general recursive dependent functions and \tit{} seem to come from the Trellys Project\cite{sjoberg2012irrelevance}.
At the time their language had several additional features not included in the surface language.
Additionally, the surface language uses a simpler notion of definitional equality resulting in a simpler proof of type soundness.
Later work in the Trellys Project\cite{casinghino2014combining,casinghino2014combiningthesis} used modalities to separate terminating and non-terminating fragments of the language, to allow both general recursion and logically sound reasoning.%, though the annotation burden seems high in retrospect .
In general, the surface language has been deeply informed by the Trellys project\cite{sjoberg2012irrelevance,casinghino2014combining,casinghino2014combiningthesis,sjoberg2015programming,sjoberg2015dependently} and the Zombie language\footnote{https://github.com/sweirich/trellys} it produced.

\todo{type in type as a well known shortcut, videos of Andraj, Conner MB saying it }
\todo{section on \bidir{} dependent type systems, citing http://www.cse.chalmers.se/\textasciitilde nad/publications/altenkirch-et-al-flops2010.pdf https://www.seas.upenn.edu/\textasciitilde sweirich/papers/congruence-extended.pdf posibly https://arxiv.org/pdf/1203.4716.pdf https://drops.dagstuhl.de/opus/volltexte/2021/13919/pdf/LIPIcs-ITP-2021-24.pdf}

\subsection{Implementations}

Several programming language implementations support features of the surface language without a proof of type soundness.
Pebble\cite{10.1007/3-540-13346-1_1} was a very early language with dependent types, though conversion did not associate alpha\todo{if saying alpha need to define it} equivalent types\footnote{according to \cite{Reinhold89typecheckingis}}.
Coquand implemented an early \bidir{} algorithm to type-check a language with \tit{}\cite{COQUAND1996167}.
Cayenne\cite{10.1145/289423.289451} is a Haskell-like language that combines dependent types with \tit{} and non-termination.
$\Pi$$\Sigma$\cite{10.1007/978-3-642-12251-4_5} is a language with \tit{} and several features for a dependently typed core calculus outlined here.
Like here $\Pi$$\Sigma$ advocates seperating terminaition concerns from type soundness concerns, though I am unaware if type soundness was ever established.
Agda supports general recursion and \tit{} with compiler flags.
Idris supports similar ``unsafe'' features.

\subsection{Other Dependent Type Systems}

There are many flavors of dependent type systems that are similar in spirit to the language presented here, but maintain logical soundness at the expense of computation.

The Calculus of Constructions (\ac{CC}, CoC)\cite{10.1016/0890-5401(88)90005-3} is one of the first minimal dependent type systems.
It contains shockingly few rules, but can express a wide variety of constructions via parametric encodings.
The system does not allow \tit{}, instead type\footnote{called \textbf{prop}, for proposition} lives in a larger universe $\star:\Square$, where $\Square$ is not considered a type.
Even though the Calculus of Constructions does not allow \tit{} it is still \textbf{impredicative} in the sense that function types can quantify over $\star$ while still being in $\star$.
For instance, the polymorphic identity $id:(X:\star)\rightarrow X\rightarrow X$ has type $\star$ so the polymorphic identity can be applied to itself, $id\,\left((X:\star)\rightarrow X\rightarrow X\right)\,id$.
From the perspective of the surface language this impredictivity is modest, but still causes issues in the presence of classical logical assumptions. \todo{Cite}
Many of the examples from this chapter are adapted from examples that were first worked out for the Calculus of Constructions.

Several other systems were developed that directly extended or modified the Calculus of Constructions.
The Extended Calculus of Constructions (\ac{ECC})\cite{luo1990extended,luo1994computation}, extends the Calculus of Constructions with a predicative hierarchy of universes and dependent pair types.
The Implicit Calculus of Constructions (\ac{ICC})\cite{10.1007/3-540-45413-6_27,10.1007/978-3-540-78499-9_26} presents an extrinsic typing system\footnote{
  Sometimes called \textbf{Curry-style}, in contrast to intrinsic systems which are sometimes called \textbf{Church-style}.
  }, unlike the Type Assignment System presented in this chapter, the Implicit Calculus of Constructions allows implicit qualification over terms in addition to explicit quantification over terms (also a hierarchy of universes, and a universe of ``sets'').
Other extensions to the Calculus of Constructions that are primarily concerned with data will be surveyed in Chapter 4.

The lambda cube\todo{cite!} is a system for relating 8 interesting typed lambda caluli to each other.
Presuming terms should always depend on terms, there are 3 additional dimensions of dependency: term depending on types, types dependent on types, and types depending on terms.
The simply typed lambda calculus has only term dependency.
System F additionally allows Types to depend on types.
The Calculus of Constructions has all forms of dependency\footnote{Recommended reading Chapter 14 \cite{sorensen2006lectures}}.

Pure Type Systems (\ac{PTS})\footnote{previously called \textbf{Generalized Type Systems}} generalizes the lambda cube to allow any number of type universes with any forms of dependency.
Notably this includes the system with one type universe where \tit{}.
Universe hierarchies can also be embedded in a \ac{PTS}.
The system described in this chapter is almost a \ac{PTS}, except that it contains unrestricted recursion and the method of type annotation is different.
All pure type systems such as System F and the Calculus of Constructions have corresponding terms in the Surface Language, by renaming their type universes into the surface language type universe.

\todo{citations for PTS: (Terlouw, 1989; Berardi, 1988; Barendregt, 1991, 1992; Jutting, McKinna, and Pollack, 1994; McKinna and Pollack, 1993; Pollack, 1994).
According to TAPL}

As previously mentioned Martin L{\"o}f Type Theory (\ac{MTLL})\cite{Martin-Lof-1972} is one of the oldest frameworks for dependent type systems.
MLTT is designed to be open, so that new constructs can be added with the appropriate introduction, elimination, computation, and typing rules.
The base system comes with a predicative hierarchy of universes, and at least dependently typed functions and a propositional equality type.
The system has two flavors characterized by its handling of definitional equality.
If types are only identified by convertibility (as the system described in this chapter) it is called Intentional Type Theory (\ac{ITT}).
If the system allows proofs of equality to associate types, it is called Extensional Type Theory (\ac{ETT}).
Since \ac{MTLL} is open ended, the Calculus of Constructions can be added to it as a subsystem\cite{aspinall2004dependent,hofmann1997extensional}.

\todo{go back to Russle?}
\todo{solving universe constraints?}
\todo{cite the autosubst proof}



% \bidir{}
% [109] Benjamin C. Pierce and David N. Turner. Local type inference. ACM Transactions on Programming Languages and Systems, 22(1):1–44, January 2000.

% cbn
%  Vilhelm Sj¨oberg and Aaron Stump. Equality, quasi-implicit products, and large
% eliminations. In ITRS 2010: Proceedings of the 5th workshop on Intersection
% Types and Related Systems, 2010. doi: 10.4204/EPTCS.45.7.


% atosubst